{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Lectures 1*\n",
    "# Introduction, Course Overview, Some Examples, Errors and Precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |\n",
    "|:---:|\n",
    "|Some lecture content will be from [**COMPUTATIONAL PHYSICS**, 3rd Ed, 2015](http://physics.oregonstate.edu/~rubin/Books/CPbook/index.html) <br>RH Landau, MJ Paez, and CC Bordeianu <br> Copyright: <br> [Wiley-VCH, Berlin;](http://www.wiley-vch.de/publish/en/books/ISBN3-527-41315-4/) and [Wiley & Sons, New York](http://www.wiley.com/WileyCDA/WileyTitle/productCd-3527413154.html)<br>  R Landau, Oregon State Unv, <br>MJ Paez, Univ Antioquia,<br> C Bordeianu, Univ Bucharest, 2015.<br> Support by National Science Foundation.|\n",
    "and other sources as shown.\n",
    "| |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "This course gives us the opportunity to explore computational aspects of topics in physics we didn't have time to do in other classes. \n",
    "\n",
    "It is **not** a course in programming.\n",
    "\n",
    "There are several categories of topics:\n",
    "* exact solutions to problems that, in previous course work, required approximations to get analytic solutions. Thus, we can also assess the accuracy of various approximations;\n",
    "* solutions to problems that were not readily done analytically (even with approximations);\n",
    "* dynamical systems;\n",
    "* moving beyond differential or integral equations, including iterative algorithms; and\n",
    "* data fitting and visualization.\n",
    "\n",
    "The curriculum has some flexibility, and I am happy to help you explore your interests.\n",
    "\n",
    "## Outline\n",
    "* [Syllabus in the Files section on Canvas](https://canvas.ucsc.edu/courses/21972/files), course tour and plan\n",
    "    -  homework, project, exam\n",
    "    - climate\n",
    "    - coding, planning your time, what to do if you are stuck\n",
    "        * it is often not easy to estimate how long something will take.\n",
    "        * **everyone** makes programming errors. Some ways to catch and correct bugs:\n",
    "            - limiting cases in which you know the answer your should get\n",
    "            - check the behavior, is it sensible? Use your physics judgement.\n",
    "            - check intermediate results as you develop the code. No shame in test print statements to debug. Plots.\n",
    "            - light of day: the more eyes the better\n",
    "            - ... what else? ...\n",
    "    - topics (and your interests!)\n",
    "    - more about projects. Step 1 is the proposal, see [the assignment in Canvas](https://canvas.ucsc.edu/courses/21972/assignments). I recommend we discuss your idea well in advance of the proposal due date, particularly if it's a new direction. Getting the scope right is important.\n",
    "* Introduction to Jupyter notebooks\n",
    "    - pros and cons of notebooks\n",
    "         -- they are not \"just\" for pedagogy. They will be a key element in the [LSST science platform](https://www.slideshare.net/MarioJuric/what-to-expect-of-the-lsst-archive-the-lsst-science-platform). We use them to communicate intermediate analysis results in current projects (e.g., see [here](https://github.com/smr456/Rankings/blob/master/Rank_ITL_analysis-v2.ipynb) and [here](https://github.com/smr456/QE-working-area/blob/master/QE_v3.ipynb)).\n",
    "         * notebooks know how to do $\\LaTeX$!\n",
    "         * be careful with the buttons that move cells up and down!\n",
    "         * [here](https://nbviewer.jupyter.org/github/jupyter/notebook/blob/master/docs/source/examples/Notebook/Notebook%20Basics.ipynb) is some information about notebooks\n",
    "         * ***DON'T FORGET to save your work often!!***  Check that the notebook is \"trusted\" and you can save your work. Also, back up your work frequently....there is no need to lose work.\n",
    "    - why python (and not c, c++, mathematica, swift, ...)\n",
    "* An example notebook, and some useful python tools\n",
    "    - numpy\n",
    "    - matplotlib\n",
    "    - pandas\n",
    "* Resources\n",
    "* Representations of numbers and precision issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python packages\n",
    "One of the wonderful aspects of Python is its extensibility. There are many packages available, but you must first tell Python what you want and how to interact with the packages. There are two ways to use a package: if you want everything, use the **import** command; if you want just specific methods in the package, use the **from** command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first check what we're using:\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful packages\n",
    "<u>*Matplotlib* (Mathematics Plotting Library):</u>\n",
    "A 2-D and 3-D graphics library that uses NumPy (Numerical Python), supports some rudimentary interactive graphics and animation, and\n",
    "exports figures in a variety of formats. Speed is not its strength. Similar to MATLAB’s plotting (except Matplotlib is free and doesn’t need its license renewed yearly). See Chapters 1.5.3 for examples and discussion.\n",
    "[http://matplotlib.org.](http://matplotlib.org)\n",
    "\n",
    "<u>*NumPy*: Numerical Python.</u>\n",
    "Permits the use of fast, high-level multidimensional arrays in Python,\n",
    "which are used as the basis for many of the numerical procedures in\n",
    "Python libraries. The successor to both\n",
    "*Numeric* and *Numarray*. Used by Visual and Matplotlib. *SciPy*\n",
    "extends NumPy. See Chapters 6.5, 6.5.1 and 11.2 for examples of NumPy\n",
    "array use.\n",
    "\n",
    "<u>*SciPy (Scientific Python)*</u>: A basic library for mathematics, science, and engineering. (See SciKits\n",
    "for further extensions.) Provides user-friendly and efficient numerical\n",
    "routines for linear algebra, optimization, integration, special\n",
    "functions, signal and image processing, statistics, genetic algorithms,\n",
    "ODE solutions, and others. Uses NumPy’s N-dimensional arrays but also\n",
    "extends NumPy. SciPy essentially provides wrapper for many existing\n",
    "libraries in other languages, such as LAPACK\n",
    "and FFT. The SciPy distribution usually includes Python, NumPy and f2py.\n",
    "http://scipy.org.\n",
    "\n",
    "<u>*Pandas*</u>: data structures and associated methods that physicists find intuitive and useful for data analysis. Also has some useful i/o capabilities, including reading and writing spreadsheets. See [https://pandas.pydata.org](https://pandas.pydata.org) From that website: *Python has long been great for data munging and preparation, but less so for data analysis and modeling. pandas helps fill this gap, enabling you to carry out your entire data analysis workflow in Python without having to switch to a more domain specific language like R.*\n",
    "\n",
    "There are many more. There is even a symbolic evaluation package, [SymPy](https://www.sympy.org/en/index.html). Often, packages for data analysis include python hooks (*e.g.*,[pyROOT](https://root.cern.ch/pyroot) which I don't recommend right now, unless you are using ROOT in your research).\n",
    "\n",
    "Finally, if you are stuck or would like additional information, you can always check [DuckDuckGo](https://duckduckgo.com) or Google or any search engine you like. There is the Discussion tool in Canvas, as well as Chat, and we'll schedule help sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's get some things we'll usually want: numpy and matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# the \"as\" gives us a convenient way to refer to the package methods, as you'll see below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...oh, and one more thing....\n",
    "So-called magic commands help make the notebook environment behave the way you need it. We'll point these out when we use them. See, *e.g.*, [here](https://towardsdatascience.com/the-top-5-magic-commands-for-jupyter-notebooks-2bf0c5ae4bb8). Of these, \"%matplotlib inline\" is the most useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this magic command makes plots appear within the notebook\n",
    "% matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's start hacking!\n",
    "# EasyMatPlot.py, Notebook Version\n",
    "# we already imported matplotlib.pyplot and numpy above\n",
    "\n",
    "# define figure size parameters to make figures larger than default. We'll see other ways to do this.\n",
    "figwidth=7\n",
    "figheight=4\n",
    "\n",
    "Xmin=-5.\n",
    "Xmax=5.\n",
    "Npoints=500\n",
    "DelX=(Xmax-Xmin)/Npoints\n",
    "x=np.arange(Xmin,Xmax,DelX)    #Form x array in range with increment\n",
    "y=np.sin(x*x)           # y array= function of x array\n",
    "#print('arange => x[0],x[1], x[499]=%8.2f %8.2f %8.2f'%(x[0],x[1],x[499]))\n",
    "#print('arange => y[0],y[1], y[499]=%8.2f %8.2f %8.2f'%(y[0],y[1],y[499]))\n",
    "fig = plt.figure(1,[figwidth,figheight]) \n",
    "ax = fig.add_subplot(111) # see https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html \n",
    "ax.set_xlabel('x')                 #labels\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('f(x) vs x')\n",
    "ax.text(-.7, -0.75,'MatPlotLib \\n Example')\n",
    "ax.plot(x,y,'-',lw=2)\n",
    "ax.grid(True)                  # form grid\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first numerical try\n",
    "This is a problem I gave in Physics 5C: a numerical treatment of a simple RC circuit. The capacitor is charged, then when the switch is closed, the voltage across the resistor = the voltage across the capacitor: $IR=Q/C$, with $I=-\\frac{dQ}{dt}$ and $Q$ is the charge on the capacitor at any time, $t$. Thus, the differential equation we are solving is $$dQ=-\\frac{1}{RC}Q(t) dt$$ Here's the plan: start with a value of $Q$, then in small time increments, calculate $dQ$ numerically (really $\\Delta Q$) in a small increment of time, $dt$ (really $\\Delta t$), and update the value of $Q$, *i.e.*, $$Q(t+\\Delta t)=Q(t)+\\Delta Q$$ and do this for small steps of $\\Delta t$. \n",
    "\n",
    "You already know that if the change in a physical quantity is proportional to the current value of that physical quantity, there will be an exponential. \n",
    "\n",
    "*BUT, there is no value of $e$ in the loop.* We  plot the exponential function on top of the data points to check that it indeed follows an exponential. Good to see!\n",
    "\n",
    "Notice what happens when the time increments become larger. \n",
    "\n",
    "**This naïve method raises many questions about numerical techniques! What are some of them?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a discharging RC circuit\n",
    "# define figure size parameters to make figures larger than default (there are other ways)\n",
    "% matplotlib notebook\n",
    "% matplotlib notebook \n",
    "#this is an interesting bug: jupyter has some issues with switching the \"back end\".\n",
    "# giving the magic command twice seems to avoid (or you could rerun the cell)\n",
    "figwidth=10\n",
    "figheight=10\n",
    "ttot=3.0 #seconds\n",
    "deltat=0.1   \n",
    "deltat=0.01   # TRY CHANGING THIS!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "RC=1.0\n",
    "Q0=1.\n",
    "time=np.arange(0.,ttot,deltat)\n",
    "#print(time)\n",
    "Q=time*1. # trick to dimension Q same as t, which we will then overwrite\n",
    "expon=time*1.\n",
    "Q[0]=Q0\n",
    "#print(Q)\n",
    "fig1=plt.figure(1,[figwidth,figheight])\n",
    "for i in range(1,len(time)): # recall, indexing counts starting from 0\n",
    "    deltaQ=-(1/RC)*Q[i-1]*deltat\n",
    "    Q[i]=Q[i-1]+deltaQ\n",
    "expon=np.exp(-time/RC) # let's compare with the included math function for an exponential\n",
    "#print (expon)\n",
    "ax=fig1.gca()\n",
    "# Major and minor ticks\n",
    "major_ticks = np.arange(0., 1.1, .2)\n",
    "minor_ticks = np.arange(0., 1.1, .1)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(True, which='both')\n",
    "fig1.suptitle('Q vs t', fontsize=16)\n",
    "plt.scatter(time, Q, 10, 'b',label='Q(t)')\n",
    "plt.plot(time, expon,'r',linewidth=3,label='expfn',alpha=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "fig1.savefig('pset4-prob12.pdf') # here's how you save plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also animate.\n",
    "This will be very useful as we chart trajectories through phase space and watch the evolution of oscillators, pendula, and orbits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "marker1, = ax.plot([], [], 'o-', markersize=10)\n",
    "\n",
    "def init1():\n",
    "    marker1.set_data([], [])\n",
    "    return marker1\n",
    "\n",
    "\n",
    "def animate1(i):\n",
    "    marker1.set_data(time[i], Q[i])\n",
    "    return marker1\n",
    "\n",
    "ani = animation.FuncAnimation(fig1, animate1,\n",
    "                            interval=10, frames=len(Q), blit=True, init_func=init1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and structure\n",
    "It's generally good to write code in a way that can be easily re-used. In particular, try to define functions (being a good OO language, you can also easily define classes, which we will do later). \n",
    "\n",
    "Here's a nice example of iterative algorithms and simple code structure that enables the user to explore the Mandelbrot set: the set of values, $c$, in the complex plane such that, starting with $z_0 =0$, $$z_{n+1}=z_n^2+c$$ remains bounded. It is found that when $|z|>2$ subsequent iterations will blow up, so color maps are made in the neighborhood of the Mandelbrot set indicating the value of $n$ for which $|z|>2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting with http://people.duke.edu/~ccc14/sta-663-2017/02_Functions.html, with some additional improvements\n",
    "def mandel(c, z=0, max_iter=100):\n",
    "    for k in range(max_iter):\n",
    "        z = z*z + c\n",
    "        if abs(z) > 2:\n",
    "            return k\n",
    "    return k\n",
    "\n",
    "def mandelbrot(w, h, xl=-1.5, xu=0.5, yl=-1, yu=1):\n",
    "    img = np.zeros((h, w)).astype('int')\n",
    "    for i, real in enumerate(np.linspace(xl, xu, w)):\n",
    "        for j, imag in enumerate(np.linspace(yl, yu, h)):\n",
    "            c = complex(real, imag)\n",
    "            img[j, i] = mandel(c)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "% matplotlib inline\n",
    "# see note above about juypter pyplot back end\n",
    "img = mandelbrot(w=400, h=400)\n",
    "fig_size = plt.rcParams[\"figure.figsize\"] # here's another way to set the figure size\n",
    "# print(fig_size)\n",
    "fig_size[0] = 10\n",
    "fig_size[1] = 10\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.grid(False)\n",
    "plt.imshow(img, cmap=plt.cm.jet, extent=(-1.5, .5, -1., 1.))\n",
    "#plt.colorbar(shrink=.8)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can use the functions to explore\n",
    "xl=-1.5\n",
    "xu=-1.2\n",
    "yl=-0.1\n",
    "yu=0.1\n",
    "img = mandelbrot(w=400, h=800,xl=xl,xu=xu,yl=yl, yu=yu)\n",
    "plt.grid(False)\n",
    "plt.imshow(img, cmap=plt.cm.jet, extent=(xl, xu, yl, yu))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl=-0.75\n",
    "xu=-0.745\n",
    "yl=0.1\n",
    "yu=0.11\n",
    "img = mandelbrot(w=400, h=800,xl=xl,xu=xu,yl=yl, yu=yu)\n",
    "plt.grid(False)\n",
    "plt.imshow(img, cmap=plt.cm.jet, extent=(xl,xu,yl,yu))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a remarkable zoom (different color scheme), from the [wikipedia page](https://en.wikipedia.org/wiki/Mandelbrot_set) on the Mandelbrot set:\n",
    "![alt text](Mandelbrot_sequence_new.gif \"\")\n",
    "\n",
    "**WARNING**: this is endlessly fascinating, so manage your time deliberately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> this is endlessley fascinating, so manage your time deliberately.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice, the cell above is a markdown. you can also include images in code cells\n",
    "#from IPython.display import Image\n",
    "#Image(filename='Mandelbrot_sequence_new.gif') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first look at solving differential equations numerically\n",
    "Let's look at the Van der Pol equation and its [limit cycles](https://en.wikipedia.org/wiki/Limit_cycle) \n",
    "$$\\ddot{x}=-\\omega_0^2x-\\mu(x^2-l_0^2)\\dot{x}$$\n",
    "\n",
    "where $\\mu$ is a positive constant. Notice that $l_0$ sets a characteristic length: when, $|x|>l_0$, then the $\\dot{x}$ term provides damping (opposes the motion), whereas when $|x|<l_0$ the velocity-dependent force is along the direction of motion (a.k.a. anti-damping). This non-linear equation was first considered by van der Pol ca. 1920, when he was studying circuits with vaccum tubes and iron-core inductors, but [the equation has been useful in other applications](https://en.wikipedia.org/wiki/Van_der_Pol_oscillator) since then, including in biophysics and seismology. Let's look at the phase space behavior when we start $x$ in the two different regions (and at $\\dot{x}=0$) for different scales of the dimensionless parameter \n",
    "\n",
    "$$\\xi=\\frac{2\\pi\\mu l_0^2}{\\omega_0}$$.\n",
    "\n",
    "See *Newtonian Dynamics*, by Ralph Baierlein, McGraw-Hill (1983), Ch 3.3. (Note: there are several Santa Cruz connections!)\n",
    "\n",
    "---\n",
    "\n",
    "We'll start with a very naïve numerical implementation, and we will develop **much better** ways to do this. \n",
    "\n",
    "What are some of the potential problems here?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vander(xnow, dt, xdnow, xi, omega0):\n",
    "# a VERY naïve implementation! We will develop much better ways to do this. What are some of the potential problems here?\n",
    "    mu=xi*omega0/(2*np.pi)  # we scale L0=1 without any loss of generality\n",
    "    xdd=-omega0*omega0*xnow-mu*(xnow*xnow-1)*xdnow\n",
    "    xdnew=xdnow+xdd*dt\n",
    "    xnew=xnow+xdnew*dt\n",
    "    return xnew, xdnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "figwidth=10\n",
    "figheight=10\n",
    "# here are the parameters will will try\n",
    "# starting position in phase space:\n",
    "x0=.03\n",
    "xd0=0.\n",
    "# and xi\n",
    "xi=1.*np.pi # explore for xi<<1 through large xi>2pi\n",
    "#xi=0.001*np.pi # explore for xi<<1 through large xi>2pi\n",
    "#\n",
    "omega0=1. # this parameter doesn't matter much -- it just sets a timescale, which we can easily scale\n",
    "period=2.*np.pi/omega0\n",
    "nperiods=9. # over how many periods will we let the oscillator run?\n",
    "ttot=nperiods*period \n",
    "deltat=0.002*period\n",
    "time=np.arange(0.,ttot,deltat)\n",
    "x = np.zeros(len(time)).astype('float')\n",
    "xd = np.zeros(len(time)).astype('float')\n",
    "x[0]=x0\n",
    "xd[0]=xd0\n",
    "for i in range(1,len(time)): # recall, indexing counts starting from 0    \n",
    "    x[i],xd[i]=vander(x[i-1],deltat,xd[i-1],xi,omega0)\n",
    "fig1=plt.figure(1,[figwidth,figheight])\n",
    "ax=fig1.gca()\n",
    "ax.grid(True)\n",
    "fig1.suptitle('Van der Pol', fontsize=16)\n",
    "plt.scatter(x, xd, 10, 'b')\n",
    "ax.set_xlabel('x', size=30)                 #labels\n",
    "ax.set_ylabel('$\\dot{x}$',size=30)\n",
    "#ax.set_title('f(x) vs x')\n",
    "x2 = np.zeros(len(time)).astype('float')\n",
    "xd2 = np.zeros(len(time)).astype('float')\n",
    "#x2[0]=2.5\n",
    "#xd2[0]=xd0\n",
    "x2[0]=2.5\n",
    "xd2[0]=.0\n",
    "for i in range(1,len(time)): # recall, indexing counts starting from 0    \n",
    "    x2[i],xd2[i]=vander(x2[i-1],deltat,xd2[i-1],xi,omega0)\n",
    "plt.scatter(x2, xd2, 10, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "marker1, = ax.plot([], [], 'o-', markersize=10)\n",
    "\n",
    "def init1():\n",
    "    marker1.set_data([], [])\n",
    "    return marker1\n",
    "\n",
    "\n",
    "def animate1(i):\n",
    "    marker1.set_data(x[i], xd[i])\n",
    "    return marker1\n",
    "\n",
    "ani = animation.FuncAnimation(fig1, animate1,\n",
    "                            interval=1, frames=len(time), blit=True, init_func=init1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, there are some serious problems with this simple implementation. We will see how to do much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "Now, let's try out our toy method on a system we know well, just to verify: 2d projectile motion. We'll see a potential problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the projectile, compared with the 2-d kinematics equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "figwidth=10\n",
    "figheight=10\n",
    "v0=10.\n",
    "theta0=45.*np.pi/180.\n",
    "g=9.8\n",
    "ttot=2.*v0/g\n",
    "deltat=0.00002*ttot \n",
    "#deltat=0.02 # looks good for small deltat, but look what happens when we make deltat just 0.02. The accumlating error would be a disaster for an oscillator.\n",
    "time=np.arange(0.,ttot,deltat)\n",
    "x = np.zeros(len(time)).astype('float')\n",
    "y = np.zeros(len(time)).astype('float')\n",
    "vx = np.zeros(len(time)).astype('float')\n",
    "vy = np.zeros(len(time)).astype('float')\n",
    "ycalc = np.zeros(len(time)).astype('float')\n",
    "vx[0]=v0*np.cos(theta0)\n",
    "vy[0]=v0*np.sin(theta0)\n",
    "for i in range(1,len(time)): # recall, indexing counts starting from 0    \n",
    "    if (y[i-1]>=0.):\n",
    "        vx[i]=vx[i-1]\n",
    "        vy[i]=vy[i-1]-g*deltat\n",
    "        x[i]=x[i-1]+vx[i-1]*deltat\n",
    "        y[i]=y[i-1]+vy[i-1]*deltat\n",
    "        ytest=y[0]+vy[0]*time[i]-0.5*g*time[i]**2\n",
    "        if (ytest>=0):\n",
    "            ycalc[i]=ytest\n",
    "        else:\n",
    "            ycalc[i]=ycalc[i-1]\n",
    "            x[i]=x[i-1]\n",
    "            y[i]=y[i-1]\n",
    "fig1=plt.figure(1,[figwidth,figheight])\n",
    "ax=fig1.gca()\n",
    "ax.grid(True)\n",
    "fig1.suptitle('y vs x', fontsize=16)\n",
    "plt.scatter(x, y, 10, 'b')\n",
    "plt.plot(x, ycalc,'r',linewidth=3)\n",
    "ax.set_xlabel('$x$', size=30)                 #labels\n",
    "ax.set_ylabel('$y$',size=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two remarkable things here:\n",
    "1) we see the trajectories match fairly well, and there is nothing explicitly parabolic in the numerical algorithm...yet it is there.\n",
    "2) notice how the numerical errors start accumulating. So, we have some important work to do.\n",
    "\n",
    "---------------\n",
    "---------------\n",
    "---------------\n",
    "\n",
    "# Errors\n",
    "\n",
    "There are several types of greatest importance here (beyond mistakes in an equation, or application of an equation outside its domain of validity, or bugs):\n",
    "  \n",
    "  - accumulation of error in an iterative algorithm. This can be catastropic (sometimes called an \"instability\").\n",
    "  - algorithmic approximation, for example truncation of an infinite series to a finite number of terms that is practical. See problem set 1, problem 4.\n",
    "  - numerical overflow or underflow: due to finite word length, there is a limit to the size of a raw number the computer can handle. See problem set 1, problem 4.\n",
    "  - Round-off errors, including (but there are others we'll see):\n",
    "    - subtractive error: due to finite machine precision, the difference between two large numbers that are almost identical can have a huge error. See problem set 1, problem 2.\n",
    "    - additive error: due to finite machine precision, adding many small numbers sequentially to a large number can lead to a large error. See problem set 1, problem 3.\n",
    "  \n",
    "We'll soon see that the differential equation solver error we encountered is dominated by an issue inherent in the approximation we used, and that it is possible to do much better with a little effort.\n",
    "\n",
    "In some cases, more than one type of error may be relevant, even an interplay among several. For example, for an infitite series, plotting the precision as a function of terms included in a series could indicate that the error is first dominated by the limited number of terms, but eventually there are numerical round-off and/or accumulation errors that take over. At that point, it makes little sense to continue to add terms in the series. You'll see this error trajectory in Problem set 1, and we'll do an example later in this notebook, too.\n",
    "\n",
    "-------\n",
    "\n",
    "-------\n",
    "\n",
    "\n",
    "\n",
    "Before we explore the other types of errors, let's sharpen our understanding of machine precision and representation of numbers.\n",
    "\n",
    "READ about machine precision, section 2.4 of your textbook. Don't worry too much about IEEE format details, if you are not interested, but pay careful attention to the bottom-line: computers have finite numerical precision -- we must know the limitations and the implications for how we want to represent a physical system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the sizes of numbers that can be handled. \n",
    "\n",
    "First the integers. The highest-order bit is the sign bit, and the rest of the bits represent the number. Thus, a 64 bits can represent a positive number as big as $$2^{63}-1. How big is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2**63-1)\n",
    "#You can ask your system what it can do:\n",
    "print(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the modern era of 64-bit computing: in Python 3, there is no `long` type, just `int`, but `numpy` still has the old-style `int32` and `int64` types, as well as 'int'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.int64(9223372036854775807)\n",
    "# np.int64(9223372036854775808)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so beware, particularly with legacy code. Python3 continues to add bits to the representation of an int if you need it.\n",
    "\n",
    "If you're interested in examining how numbers are stored, recall hexadecimal representation: each group of 4 bits can represent a number (0,1,2,...,9,10,11, 12, 13, 14, 15), which is written as (0, 1, 2, ...,9, a, b, c, d, e, f). A byte is 8 bits, so all 8 bits =1 would be ff, and the largest positive int would be all ff, except the highest byte which would be 7f (sign bit is 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hex(0),hex(1), hex(15), hex(sys.maxsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the important point here is that, with fixed decimal numbers, such as 'int', the smaller numbers have relatively larger representation error (the least-significant bit is a larger fraction of smaller numbers). \n",
    "\n",
    "Now the floats. The relevant quantities are the mantissa and the exponent. You can ask the system for its limitations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.float_info,'\\n')\n",
    "print(\"For numpy \",np.finfo(np.float))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical operations adjust accordingly. For example, if you add a bit and a small number, the system must first match the exponents, so you can run out of mantissa bits. Feel free to read the details in the textbook, but the upshot is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1.000000000000001)\n",
    "print(1.0000000000000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you see there can be a problem when you add very small numbers to very large numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=1.\n",
    "y=1.\n",
    "for i in range(0,10):\n",
    "    x+=0.000000000000001\n",
    "    y+=0.0000000000000001\n",
    "    print(x,'          ',y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's do this 10 million times   \n",
    "x=1.\n",
    "y=1.\n",
    "for i in range(0,10000000):\n",
    "    x+=0.000000000000001\n",
    "    y+=0.0000000000000001\n",
    "print(x,'        ',y)\n",
    "# y never gets bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lesson: expect trouble if you add many small floats to big floats. Best to sum the small numbers together first.\n",
    "This is the point of the (up) and (down) sum question in Problem set 1. One more look to make sure we see the issue...it's not the dynamic range that's the problem here, it's the machine precision of ints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1 + 1.0e-15,'\\n')\n",
    "print(1 + 1.0e-16,'\\n')\n",
    "print(1.00001e-15*1.e-16,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The textbook defines *machine precision* as \"the maximum positive number that, on the computer, can be added to the number stored as 1 without changing that stored 1.\"\n",
    "\n",
    "In other words, the machine precision $\\epsilon_\\text{m}$ is the largest $\\epsilon$ so that $1_\\text{c}+\\epsilon_\\text{m}=1_\\text{c}$. \n",
    "\n",
    "Here's the suggested code to illustrate the point:\n",
    "\n",
    "### Limits.py \n",
    "\n",
    "The following program can be used to determine the machine precision *ϵ*<sub>*m*</sub> of\n",
    "your computer system within a factor of 2.  The idea is to find out when the internal representation of `1+eps` runs out of precision to distinguish `1+eps` from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limits.py: determines machine precision \n",
    "N = 75\n",
    "eps = 1.0\n",
    "\n",
    "for i in range(N):\n",
    "    eps = eps/2\n",
    "    one_Plus_eps = 1.0  +  eps\n",
    "    print('one  +  eps = ', one_Plus_eps)\n",
    "    print('eps = ', eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are not sure: **TEST!**. \n",
    "\n",
    "As expected from when we asked the system about itself earlier, problems start to crop up around $10^{-15}$ and below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "-----------------\n",
    "\n",
    "Your book does a good job showing one way to understand the round-off effects for subtraction and division. Here it is. See sections 3.1.1 - 3.1.4.\n",
    "\n",
    "The computer representation\n",
    "$x_c$ of an exact number $x$ can be modelled as\n",
    "\n",
    "$$x_c  \\simeq  x (1 +\\epsilon_{x}),$$\n",
    "  \n",
    "where $\\epsilon_x$ is the relative error in $x_c$. ($\\epsilon_x$ is basically the machine precision.)\n",
    "\n",
    "\n",
    "### Subtraction\n",
    "\n",
    "If we attempt to perform a simple subtraction\n",
    "$$a = b − c$$\n",
    "the computer will use its representations of $a, b, c$:\n",
    "\n",
    "$$\\begin{align}\n",
    "    a_{c}  & \\simeq  b_c - c_c \\\\\n",
    "    & \\simeq b(1 + \\epsilon_{b}) - c(1 +\n",
    "    \\epsilon_c)   \\\\\n",
    "    \\frac{a_c}{a} &   \\simeq  1 +\n",
    "    \\epsilon_{b} \\frac{b}{a} - \\frac{c}{a} \\epsilon_c\n",
    " \\end{align}$$\n",
    "The resulting error in $a$ is essentially a weighted average of the relative errors in $b$ and $c$.\n",
    "\n",
    "For round-off error, the total uncertainty is 0 if the last two terms cancel, and this happens most easily if $b = c$ and $\\epsilon_b = \\epsilon_c$, but that doesn't happen, generally.\n",
    "\n",
    "As already noted, the error in the answer $a_c$ increases when we subtract two nearly\n",
    "equal numbers ($b \\simeq c$) because then we are subtracting off the most\n",
    "significant parts of both numbers and leaving the error-prone least-significant\n",
    "parts: \n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{a_c}{a} & =         1 + \\epsilon_{a} \\\\\n",
    "& \\simeq  1 + \\epsilon_{b} \\frac{b}{a} - \\frac{c}{a} \\epsilon_c \\\\\n",
    "& \\simeq 1+\n",
    "\\frac{b}{a}(\\epsilon_{b} - \\epsilon_{c})\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "This shows that, even if the relative errors in $b$ and $c$ cancel\n",
    "somewhat, they are multiplied by the large number $b/a$, which can\n",
    "significantly magnify the error. Because we cannot assume any sign for\n",
    "the errors, we must assume the worst \\[the “max” error\\].\n",
    "\n",
    "\n",
    "### Division\n",
    "\n",
    "The investigation of division is similar to the derivation for subtraction:\n",
    "\n",
    "$$\\begin{align}\n",
    "   a_{c} & = \\frac{b_{c}} {c_{c}} \\\\\n",
    "         & = \\frac{b(1+\\epsilon_{b})}{c(1 + \\epsilon_{c})}\\\\\n",
    "\\frac{a_{c}}{a}  & = \\frac{1+\\epsilon_{b}}{1 + \\epsilon_{c}} \\\\\n",
    " &\\simeq    (1 + \\epsilon_{b})(1 - \\epsilon_{c})\\\\\n",
    " &\\simeq 1+ \\epsilon_b -\\epsilon_c \\\\\n",
    " &\\simeq 1+ |\\epsilon_b| + |\\epsilon_c|\n",
    " \\end{align}$$\n",
    " \n",
    "The end result is that the round-off errors contribute linearly to the relative error on the product.  This makes sense because any digit in one factor is multipled by all of the other digits in the other factor.\n",
    "\n",
    "By carefully estimating the round-off error, we can see if the *round-off error* or the *approximation error* dominates in a calculation.\n",
    "\n",
    "As we already discussed, for the most common infinite series (uniform convergent power series), one can add more terms to make the approximation error arbitrarily small.  But the *round-off error* will obviously come into play as the terms become smaller and smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "As promised, here's another algorithm whose error is first dominated by including too few terms in the series (approximation error), and then by round-off errors. Explore the exponential function:\n",
    "\n",
    "$$e^{-x} = 1 − x + \\frac{x^2}{2!}−\\frac{x^3}{3!} + \\cdots$$\n",
    "\n",
    "for various values of $x$. You'll see interesting behavior as $x$ gets larger, both in the trajectory and the asymptotic error. How could you improve the asymptotic error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponential_good.py\n",
    "# deftly coded by Jason Nielsen\n",
    "x=.5   # change this and explore!\n",
    "relative_errors = []\n",
    "N_values = []\n",
    "\n",
    "exact_answer = np.exp(-x)\n",
    "#print(\"Exact answer is %f\" % exact_answer)\n",
    "\n",
    "for N in range (1, 100):\n",
    "#    print(\"N = %d\" % N)\n",
    "    sum = 1.\n",
    "    term = 1.\n",
    "    for n in range (1, N):\n",
    "        term *= -x/n\n",
    "        sum += term\n",
    "#    print(sum)\n",
    "    N_values.append(N)\n",
    "    relative_errors.append(abs((sum-exact_answer)/exact_answer))\n",
    "\n",
    "plt.plot(N_values, relative_errors)\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Relative Error')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
